{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a36b63-4beb-4a62-beff-16c1cacf8cda",
   "metadata": {},
   "source": [
    "# Understanding Large Language Models\n",
    "* Traditional methods excelled at categorization tasks such as email spam classification and straightforward pattern recognition that could be captured with handcrafted rules or simpler models. However, they typically underperformed in language tasks that demanded complex understanding and generation abilities.\n",
    "* When we say language models \"understand,\" we mean that they can process and generate text in ways that appear coherent and contextually relevant, not that they possess human-like consciousness or comprehension.\n",
    "* The success behind LLMs can be attributed to the transformer architecture which underpins many LLMs, and the vast amounts of data LLMs are trained on, allowing them to capture a wide variety of linguistic nuances, contexts, and patterns that would be challenging to manually encode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1719aaa-d14d-4e3a-8899-856859ff428e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is an LLM?\n",
    "* An LLM, a large language model, is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data.\n",
    "* The \"large\" in large language model refers to both the model's size in terms of parameters and the immense dataset on which it's trained.\n",
    "* LLM uses next-word prediction, which is sensible because it harnesses the inherent sequential nature of language to train models on understanding context, structure, and relationships within text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65753da1-fee9-4443-b953-b1ad05b120fd",
   "metadata": {},
   "source": [
    "## Applications of LLMs\n",
    "* Today, LLMs are employed for machine translation, generation of novel texts, sentiment analysis, text summarization, and many other tasks. LLMs have recently been used for content creation, such as writing fiction, articles, and even computer code.\n",
    "* LLMs can also power sophisticated chatbots and virtual assistants, such as OpenAI's ChatGPT or Google's Gemini. LLMs may also be used for effective knowledge retrieval from vast volumes of text in specialized areas such as medicine or law."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034b461-789a-40c0-ba54-fe27a9ceed9b",
   "metadata": {},
   "source": [
    "## Stages of building and using LLMs\n",
    "* Coding an LLM from the ground up is an excellent exercise to understand its mechanics and limitations. Also, it equips us with the required knowledge for pretraining or finetuning existing open-source LLM architectures to our own domain-specific datasets or tasks.\n",
    "* The general process of creating an LLM includes pretraining and finetuning.\n",
    "  * The term \"pre\" in \"pretraining\" refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language.\n",
    "  * Finetuning is a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains.\n",
    "    * In instruction-finetuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.\n",
    "    * In classification finetuning, the labeled dataset consists of texts and associated class labels, for example, emails associated with spam and non-spam labels.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c87ed-629e-488c-893d-45c4cfb4824b",
   "metadata": {},
   "source": [
    "## Using LLMs for different tasks\n",
    "* Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper Attention Is All You Need. The transformer architecture depicted in Figure 1.4 consists of two submodules, an encoder and a decoder.\n",
    "  * The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input.\n",
    "  * The decoder module takes these encoded vectors and generates the output text from them.\n",
    "* A key component of transformers and LLMs is the self-attention mechanism, which allows the model to weigh the importance of different words or tokens in a sequence relative to each other.\n",
    "* BERT is built upon the original transformer's encoder submodule. It specializes in masked word prediction, where the model predicts masked or hidden words in a given sentence. This unique training strategy equips BERT with strengths in text classification tasks.\n",
    "* GPT focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts.\n",
    "* GPT adepts at executing both zero-shot and few-shot learning tasks. Zero-shot learning refers to the ability to generalize to completely unseen tasks without any prior specific examples. On the other hand, few-shot learning involves learning from a minimal number of examples the user provides as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be935344-f356-49e7-8b2d-295b6f9e21fe",
   "metadata": {},
   "source": [
    "## Utilizing large datasets\n",
    "* From each dataset, only a fraction of the data, (amounting to a total of 300 billion tokens) was used in the training process. This sampling approach means that the training didn't encompass every single piece of data available in each dataset. Instead, a selected subset of 300 billion tokens, drawn from all datasets combined, was utilized. Also, while some datasets were not entirely covered in this subset, others might have been included multiple times to reach the total count of 300 billion tokens.\n",
    "* For context, consider the size of the CommonCrawl dataset, which alone consists of 410 billion tokens and requires about 570 GB of storage.\n",
    "* Pretraining LLMs requires access to significant resources and is very expensive. For example, the GPT-3 pretraining cost is estimated to be $4.6 million in terms of cloud computing credits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b1113-d16b-463a-929a-dd8155bd7459",
   "metadata": {},
   "source": [
    "## A closer look at the GPT architecture\n",
    "* GPT stands for Generative Pretrained Transformer and was originally introduced in the following paper:\n",
    "  * Improving Language Understanding by Generative Pre-Training (2018) by Radford et al. from OpenAI, http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "* the original model offered in ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT paper.\n",
    "* It is actually very remarkable given that GPT models are pretrained on a relatively simple next-word prediction task, but can carry out other tasks such as spelling correction, classification, or language translation.\n",
    "* The next-word prediction task is a form of self-supervised learning, which is a form of self-labeling. This means that we don't need to collect labels for the training data explicitly but can leverage the structure of the data itself: we can use the next word in a sentence or document as the label that the model is supposed to predict. \n",
    "* Since this next-word prediction task allows us to create labels \"on the fly,\" it is possible to leverage massive unlabeled text datasets to train LLMs.\n",
    "* The general GPT architecture is relatively simple, it's just the decoder part without the encoder. \n",
    "* Since decoder-style models like GPT generate text by predicting text one word at a time, they are considered a type of autoregressive model. Autoregressive models incorporate their previous outputs as inputs for future predictions. Consequently, in GPT, each new word is chosen based on the sequence that precedes it, which improves coherence of the resulting text.\n",
    "* The ability to perform tasks that the model wasn't explicitly trained to perform is called an \"emergent behavior.\" This capability isn't explicitly taught during training but emerges as a natural consequence of the model's exposure to vast quantities of multilingual data in diverse contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38543f7-b40b-46e7-bce7-0937a877bd87",
   "metadata": {},
   "source": [
    "##  Building a large language model\n",
    "* First, we will learn about the fundamental data preprocessing steps and code the attention mechanism that is at the heart of every LLM.\n",
    "* Next, in stage 2, we will learn how to code and pretrain a GPT-like LLM capable of generating new texts. And we will also go over the fundamentals of evaluating LLMs, which is essential for developing capable NLP systems.\n",
    "*  The focus of stage 2 is on implementing training for educational purposes using a small dataset.\n",
    "* Finally, in stage 3, we will take a pretrained LLM and finetune it to follow instructions such as answering queries or classifying texts -- the most common tasks in many real-world applications and research.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
